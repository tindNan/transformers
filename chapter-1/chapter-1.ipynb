{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecf9fa6-d993-49f3-b920-cba69edc562f",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cda26c-dbb0-4ccd-b0f1-0bae0cde345c",
   "metadata": {},
   "source": [
    "The most basic object in the ðŸ¤— Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d2ac1b-251a-43a1-b35f-2da81d963537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998669624328613},\n",
       " {'label': 'NEGATIVE', 'score': 0.9998083710670471}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "classifier([\"Shadrack is an amazing developer\", \"Shadrack is not funny\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e36da-b9f4-401b-8c71-8bae391b1fff",
   "metadata": {},
   "source": [
    "By default, this pipeline selects a particular pretrained model (in the above case it was the `distilbert/distilbert-base-uncased-finetuned-sst-2-english`) that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
    "\n",
    "However you can use other models from places such as hugging face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae64e0-4222-4b37-ad0d-6f3577bb3ef2",
   "metadata": {},
   "source": [
    "The pipeline also handles all the steps in between e.g. moving from human readable text, to numbers that the computer can handle and then return a meaningful output that users can understand.\n",
    "\n",
    "There are three main steps involved when you pass some text to a pipeline:\n",
    "\n",
    "1. The text is preprocessed into a format the model can understand.\n",
    "2. The preprocessed inputs are passed to the model.\n",
    "3. The predictions of the model are post-processed, so you can make sense of them.\n",
    "\n",
    "### Some of the currently available pipelines are:\n",
    "\n",
    "1. feature-extraction (get the vector representation of a text)\n",
    "2. fill-mask\n",
    "3. ner (named entity recognition)\n",
    "4. question-answering\n",
    "5. sentiment-analysis\n",
    "6. summarization\n",
    "7. text-generation\n",
    "8. translation\n",
    "9. zero-shot-classification\n",
    "\n",
    "We'll dive into a few of these, namely:\n",
    "1. zero-shot-classification\n",
    "2. text-generation\n",
    "3. ner (named entity recognition)\n",
    "4. fill-mask (Mask filling)\n",
    "5. question-answering\n",
    "6. summarization\n",
    "7. translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909c2be-d7b4-45e5-aab4-f6aa7d569f92",
   "metadata": {},
   "source": [
    "## Zero Shot Classification\n",
    "\n",
    "This is useful for classifying texts that have not yet been labelled. This quite common in the real world since it's hard to assume that the LLM has been trained on all types of data, which have been successfully labelled and annotated by domain experts. After all, new things come up every day ðŸ˜ƒ.\n",
    "\n",
    "For zero shot classification, you get to define the labels that are used during classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15abac05-302d-4ddb-bb92-d6022c93c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Today is a day where the American president has won the election and there are many people who are not happy about it',\n",
       " 'labels': ['politics', 'business', 'film', 'education', 'sports'],\n",
       " 'scores': [0.9157540798187256,\n",
       "  0.02831583097577095,\n",
       "  0.025808537378907204,\n",
       "  0.015261565335094929,\n",
       "  0.014859919436275959]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "zero_shot_classifier(\"Today is a day where the American president has won the election and there are many people who are not happy about it\", candidate_labels=[\"education\", \"politics\", \"sports\", \"business\", \"film\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8274f1d-8fce-4983-8916-fdaf971b36dd",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235dce42-ffd5-4390-be62-68ddd2710c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Tomorrow I will go out of the city, I will go to the monastery.\\n\\nI had come for that reason, I had come to the monastery because I knew that God told me to, for I was going to receive what He told me'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "print(generator(\"Tomorrow I will\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1e431-7bb8-4e58-b583-7c2a4a5aa157",
   "metadata": {},
   "source": [
    "You can control how many different sequences/versions are generated with the argument `num_return_sequences` and the total length of the output text with the argument `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1181acf-1172-4cf4-b238-618ba96240fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Tomorrow I will be at that.\"\\n\\nBoris added: \"I just hope that we get a little bit of a breakthrough at the top for a while, so I don\\'t think that it\\'s going to drag us down in terms of being better, but you know I don\\'t think we\\'ll get too far ahead of ourselves. I think we\\'ll have to find a way to put a little bit forward the more we can get some rest.\"\\n\\nArsenal have played less than 7,'},\n",
       " {'generated_text': 'Tomorrow I will tell you, you are probably just looking at the news too much.\\n\\nWhen it comes to getting out of prison, you are more than likely on some level a prison inmate. This includes the average citizen, and the prison population has grown over the past few decades. It is the reason in many cases that you are no longer the inmate at all.\\n\\nA person who has spent 20 or more years in an institution has a large portion of his life devoted to helping others'},\n",
       " {'generated_text': 'Tomorrow I will be here. I want to do this.\"'},\n",
       " {'generated_text': 'Tomorrow I will make that speech,\" said her husband, Stephen Fraser Fraser, of his father\\'s North Surrey suburb of Redhill.\\n\\n\"I want to see my mum in the next couple days.\"\\n\\nThe campaign against Giffords, which started following the death of their mother, has received more than 50,000 signatures since its announcement. The campaign has also attracted attention from other local campaigners.\\n\\nA survey of social media users by The Guardian found that 58 per cent said they'},\n",
       " {'generated_text': 'Tomorrow I will try to bring him back to the league. He has been in excellent shape, but it is nice he has got out there and is getting better each week.\"\\n\\nRuff, who had 23 goals last season and 17 so far, has missed part of the first two months with a shoulder strain.'},\n",
       " {'generated_text': 'Tomorrow I will send you to my sister\\'s house,\" he says in his journal.\\n\\nHe doesn\\'t tell her that. She doesn\\'t come to see him anymore, when he will be gone for the rest of the month. He would never tell her that. When he was younger, he never told her that. When he would tell her that, she wanted him, but he insisted she wouldn\\'t let him see her. She told him he wouldn\\'t do that. So when she did'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Tomorrow I will\", max_length=100, num_return_sequences=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd75870-ea47-425a-90b4-667cb547d292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|                                                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "swahili_pipeline = pipeline(\"text-generation\", model=\"Jacaranda/UlizaLlama3\")\n",
    "\n",
    "swahili_pipeline(\"Kesho nitaenda shule halafu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
